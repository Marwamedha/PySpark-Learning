{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8f18b6c1-823f-4517-a5e0-2e818a55c671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Data Reading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bea6cbc-2713-46c2-8777-7d3bcdf5091f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[6]: [FileInfo(path='dbfs:/FileStore/tables/sample.txt', name='sample.txt', size=59, modificationTime=1747504110000)]"
     ]
    }
   ],
   "source": [
    "# to get the file name \n",
    "dbutils.fs.ls('/FileStore/tables/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fe84d569-cb2b-4524-bbe8-f75b276c00da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df =spark.read.format('csv').option('inferSchema',True).option('header',True).load('/FileStore/tables/sample.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2803968-1845-4b16-b9a1-c4c943172230",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from pyspark import Row\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a8c3851-cdc3-4afd-bed7-d8bc586a49c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RDD elements: [1, 2, 3, 4, 5]\nFile contents: ['Hello world', 'Hello Spark', 'This is an RDD lab', 'RDDs are cool']\nSquares: [1, 4, 9, 16, 25]\nEven numbers: [2, 4]\nWords: ['Hello', 'world', 'Hello', 'Spark', 'This', 'is', 'an', 'RDD', 'lab', 'RDDs', 'are', 'cool']\nUnique words: ['Hello', 'world', 'Spark', 'is', 'an', 'lab', 'are', 'This', 'RDD', 'RDDs', 'cool']\nCount: 5\nSum: 15\nFirst element: 1\nTake 3: [1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .master(\"local[4]\")\\\n",
    "    .appName(\"RDDs\")\\\n",
    "    .config(\"spark.eventLog.logBlockUpdates.enabled\", True)\\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Creating a RDD from a collection\n",
    "data = [1, 2, 3, 4, 5]\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "print(\"RDD elements:\", rdd.collect())\n",
    "\n",
    "\n",
    "# Creating a RDD from a text file\n",
    "text_rdd = spark.sparkContext.textFile(\"/FileStore/tables/sample.txt\")\n",
    "print(\"File contents:\", text_rdd.collect())\n",
    "\n",
    "# RDD Transformations\n",
    "# map\n",
    "squared = rdd.map(lambda x: x ** 2)\n",
    "print(\"Squares:\", squared.collect())\n",
    "\n",
    "# filter\n",
    "even = rdd.filter(lambda x: x % 2 == 0)\n",
    "print(\"Even numbers:\", even.collect())\n",
    "\n",
    "# flatmap\n",
    "words = text_rdd.flatMap(lambda line: line.split(\" \"))\n",
    "print(\"Words:\", words.collect())\n",
    "\n",
    "# distinct\n",
    "unique_words = words.distinct()\n",
    "print(\"Unique words:\", unique_words.collect())\n",
    "\n",
    "# RDD Actions\n",
    "print(\"Count:\", rdd.count())\n",
    "print(\"Sum:\", rdd.sum())\n",
    "print(\"First element:\", rdd.first())\n",
    "print(\"Take 3:\", rdd.take(3))\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "My Project",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66b1737c-bedb-4713-9502-d1ac00fde5f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca7270ae-9fa9-4b40-aeac-fb4240d5b7b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------+\n|   name|   category|amount|\n+-------+-----------+------+\n|  Alice|Electronics|   300|\n|    Bob|   Clothing|   100|\n|  Alice|   Clothing|   150|\n|    Bob|Electronics|   200|\n|Charlie|   Clothing|    50|\n|Charlie|Electronics|   400|\n+-------+-----------+------+\n\n+-----------+\n|sum(amount)|\n+-----------+\n|       1200|\n+-----------+\n\n+-----------+\n|avg(amount)|\n+-----------+\n|      200.0|\n+-----------+\n\n+-----------+-----------+\n|   category|sum(amount)|\n+-----------+-----------+\n|Electronics|        900|\n|   Clothing|        300|\n+-----------+-----------+\n\n+-------+-----+\n|   name|count|\n+-------+-----+\n|  Alice|    2|\n|    Bob|    2|\n|Charlie|    2|\n+-------+-----+\n\n+-----------+-----------+\n|   category|max(amount)|\n+-----------+-----------+\n|Electronics|        400|\n|   Clothing|        150|\n+-----------+-----------+\n\n+-----------+\n|total_sales|\n+-----------+\n|       1200|\n+-----------+\n\n+----------+\n|avg_amount|\n+----------+\n|     200.0|\n+----------+\n\n+-----------+-----+\n|   category|total|\n+-----------+-----+\n|Electronics|  900|\n|   Clothing|  300|\n+-----------+-----+\n\n+-------+--------+\n|   name|tx_count|\n+-------+--------+\n|  Alice|       2|\n|    Bob|       2|\n|Charlie|       2|\n+-------+--------+\n\n+-----------+----------+\n|   category|min_amount|\n+-----------+----------+\n|Electronics|       200|\n|   Clothing|        50|\n+-----------+----------+\n\n+-------+-----------+\n|   name|sum(amount)|\n+-------+-----------+\n|  Alice|        450|\n|Charlie|        450|\n+-------+-----------+\n\n+-------+-----+\n|   name|total|\n+-------+-----+\n|  Alice|  450|\n|Charlie|  450|\n+-------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count, min, max\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Aggregation Lab\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample data\n",
    "sales_data = [\n",
    "    (\"Alice\", \"Electronics\", 300),\n",
    "    (\"Bob\", \"Clothing\", 100),\n",
    "    (\"Alice\", \"Clothing\", 150),\n",
    "    (\"Bob\", \"Electronics\", 200),\n",
    "    (\"Charlie\", \"Clothing\", 50),\n",
    "    (\"Charlie\", \"Electronics\", 400),\n",
    "]\n",
    "\n",
    "# Define schema\n",
    "columns = [\"name\", \"category\", \"amount\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(sales_data, columns)\n",
    "\n",
    "# Show raw data\n",
    "df.show()\n",
    "\n",
    "# Register as SQL table\n",
    "df.createOrReplaceTempView(\"sales\")\n",
    "# Total sales\n",
    "df.groupBy().sum(\"amount\").show()\n",
    "\n",
    "# Average sale amount\n",
    "df.groupBy().avg(\"amount\").show()\n",
    "\n",
    "# Total per category\n",
    "df.groupBy(\"category\").sum(\"amount\").show()\n",
    "\n",
    "# Count purchases per customer\n",
    "df.groupBy(\"name\").count().show()\n",
    "\n",
    "# Max spend by category\n",
    "df.groupBy(\"category\").max(\"amount\").show()\n",
    "# Total sales using SQL\n",
    "spark.sql(\"SELECT SUM(amount) as total_sales FROM sales\").show()\n",
    "\n",
    "# Average sale amount\n",
    "spark.sql(\"SELECT AVG(amount) as avg_amount FROM sales\").show()\n",
    "\n",
    "# Total per category\n",
    "spark.sql(\"SELECT category, SUM(amount) as total FROM sales GROUP BY category\").show()\n",
    "\n",
    "# Count of transactions per user\n",
    "spark.sql(\"SELECT name, COUNT(*) as tx_count FROM sales GROUP BY name\").show()\n",
    "\n",
    "# Min spend by category\n",
    "spark.sql(\"SELECT category, MIN(amount) as min_amount FROM sales GROUP BY category\").show()\n",
    "# Who spent more than 400 in total?\n",
    "df.groupBy(\"name\").sum(\"amount\").filter(col(\"sum(amount)\") > 400).show()\n",
    "\n",
    "# SQL equivalent\n",
    "spark.sql(\"\"\"\n",
    "SELECT name, SUM(amount) as total \n",
    "FROM sales \n",
    "GROUP BY name \n",
    "HAVING total > 400\n",
    "\"\"\").show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-05-17 15:59:16",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}